env: "healthy"          # Which model to import ("healthy" or "prosthesis").      

train_mode: True        # If you want to train the agent, set this to True. If false, no changes will be made to the model.
visualize: False        # If you want to display the image, set this to True. Turn this off if you run this on Peregrine/Google Collab.
log_wandb: False         # Enables logging in wandb.

n_update: 300           # How many episodes before the Policy is updated (per worker). Also regarded as the simulation budget (steps) per iteration.
n_aux_update: 5         # How many episodes before the Auxiliary is updated.
num_workers: 5          # How many agents you want to run asynchronously.

policy_kl_range: 0.03   # Recommended to set it to 0.03 for Continuous action spaces.
policy_params: 5        # Recommended set to 5 for Continuous
value_clip: 1.0         # How many value will be clipped. Recommended set to the highest or lowest possible reward
entropy_coef: 0.0       # How much action randomness is introduced. Because we use Standard Deviation for Continuous, no need to use Entropy for randomness.
vf_loss_coef: 1.0       # Just set to 1.
batch_size: 32          # How many batches per update. size of batch = n_update / batch_size. Recommended to set it to 32 for Continuous.
PPO_epochs: 10          # How many epochs per update. Recommended to set it to 10 for Continuous.    

gamma: 0.99             # Just set to 0.99
lam: 0.95               # Just set to 0.95
learning_rate: 2.5e-4   # Just set to 0.00025
