method: bayes
metric:
  name: info.total_reward
  goal: maximize
parameters:
  env:
    value: healthy
  train_mode: 
    value: True
  visualize: 
    value: False
  log_wandb:
    value: True
  PPO_epochs:
    distribution: int_uniform
    min: 5
    max: 20
  n_update:
    distribution: int_uniform
    min: 1000
    max: 10000
  n_aux_update:
    distribution: int_uniform
    min: 2
    max: 20
  batch_size:
    distribution: int_uniform
    min: 16
    max: 64
  policy_params:
    distribution: int_uniform
    min: 2
    max: 10
  lam:
    distribution: uniform
    min: 0.3
    max: 0.99
  gamma:
    distribution: uniform
    min: 0.3
    max: 0.99
  value_clip:
    distribution: uniform
    min: 0.5
    max: 2.0
  vf_loss_coef:
    distribution: uniform
    min: 0.5
    max: 2.0
  learning_rate:
    distribution: uniform
    min: 0.000125
    max: 0.001
  policy_kl_range:
    distribution: uniform
    min: 0.01
    max: 0.06
  entropy_coef:
    distribution: uniform
    min: 0.0
    max: 0.2
controller:
  type: local
# program: src.sweep_mpi    
# command:
#   - mpirun
#   - "--mca" 
#   - opal_warn_on_missing_libcuda
#   - 0
#   - python
#   - "-m"
#   - ${program}
#   - ${args}