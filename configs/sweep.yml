method: bayes
metric:
  name: avg_reward
  goal: maximize
parameters:

  # OpenSim
  env:
    value: healthy
  data:
    value: AB06

  # Logging
  visualize: 
    value: False
  log_wandb:
    value: True

  # Worker
  train_mode: 
    value: True
  n_steps:
    values: [1024, 2048, 4096]

  # TrulyPPO
  n_ppo_epochs:
    values: [2, 4, 8, 16]
  ppo_batch_size:
    values: [16, 32, 48, 64]
  ppo_delta:
    distribution: uniform
    min: 0.01
    max: 0.08
  ppo_alpha:
    distribution: uniform
    min: 1.0
    max: 10.0
  initial_logstd:
    distribution: uniform
    min: -3
    max: 0

  # Critic (PPO)
  value_clip:
    distribution: uniform
    min: 0.5
    max: 2.0
  entropy_coef:
    distribution: uniform
    min: 0.0
    max: 0.2
  vf_loss_coef:
    distribution: uniform
    min: 0.5
    max: 2.0

  # Auxiliary
  n_aux_update:
    values: [4, 8, 16, 32]
  n_aux_epochs:
    values: [2, 4, 8, 16]

  # Optimization
  lam:
    values: [0.99, 0.999]
  gamma:
    value: 0.99
  learning_rate:
    distribution: uniform
    min: 0.000125
    max: 0.001

controller:
  type: local
# program: src.sweep_mpi    
# command:
#   - mpirun
#   - "--mca" 
#   - opal_warn_on_missing_libcuda
#   - 0
#   - python
#   - "-m"
#   - ${program}
#   - ${args}