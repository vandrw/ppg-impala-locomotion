method: bayes
metric:
  name: avg_reward
  goal: maximize
parameters:

  # OpenSim
  env:
    value: healthy
  data:
    value: AB06

  # Logging
  visualize: 
    value: False
  log_wandb:
    value: True

  # Worker
  train_mode: 
    value: True
  n_steps:
    values: [1024, 2048, 4096]

  # TrulyPPO
  ppo_epochs:
    values: [2, 4, 8, 16]
  ppo_batchsize:
    values: [16, 32, 48, 64]
  ppo_kl_range:
    distribution: uniform
    min: 0.01
    max: 0.08
  slope_rollback:
    distribution: uniform
    min: 0.0
    max: 7.0
  slope_likelihood:
    values: [0, 1]
  initial_logstd:
    distribution: uniform
    min: -3
    max: 0

  # Critic (PPO)
  val_clip_range:
    distribution: uniform
    min: 0.3
    max: 1.0
  entropy_coef:
    values: [0.0, 0.05, 0.1, 0.15]
  vf_loss_coef:
    distribution: uniform
    min: 0.5
    max: 2.0

  # Auxiliary
  aux_update:
    values: [4, 8, 16, 32]
  aux_epochs:
    values: [2, 4, 8, 16]
  aux_batchsize:
    values: [16, 32, 64, 128]
  beta_clone:
    distribution: uniform
    min: 0.5
    max: 1.5

  # Optimization
  lambd:
    values: [0.99, 0.999]
  gamma:
    value: 0.99
  learning_rate:
    distribution: uniform
    min: 0.0001
    max: 0.001

controller:
  type: local
# program: src.sweep_mpi    
# command:
#   - mpirun
#   - "--mca" 
#   - opal_warn_on_missing_libcuda
#   - 0
#   - python
#   - "-m"
#   - ${program}
#   - ${args}